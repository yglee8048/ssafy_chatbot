# 2018-12-18

## python을 활용한 웹 크롤링

### html

```python
soup = BeautifulSoup(response, 'html.parser')

# div 중 id가 Header 인 것을 찾는다.
dt = soup.find('div',{'id':'Header'})
# 파싱된 html을 문자열로 변경할 수있다.
date = str(dt)

# 특정 요소를 select로 선택할 수 있다.
# select의 경우 무조건 list로 출력된다.
date = soup.select('#drwNoDate')[0].text
# select의 조건은 copy > copy selector 로 확인할 수 있다.
getn = soup2.select('#container > div > div.tbl_basic > table > tbody > tr')

# select를 사용해서 css 태그를 검색하면 쉽게 크롤링이 가능하다.
'title' : item.select('dt a')[0].text,

```



### json

```python
# json 파일을 파싱하는 방법
import json
response = requests.get(url).text
output = json.loads(response)

# json은 파싱하고 나면 dict 타입이기 때문에 크롤링이 쉽다.
'title' : docu['data']['title']

```





## 챗봇 소스 모음

### 로또결과

```python
import requests
from bs4 import BeautifulSoup

url = 'https://m.dhlottery.co.kr/common.do?method=main'
response = requests.get(url).text
soup = BeautifulSoup(response, 'html.parser')
getn = soup.find('div', {'class':'prizeresult'})
lst = getn.find_all('span')

ns = []
for x in lst:
  ns.append(int(x.text))
  
date = soup.select('#drwNoDate')[0].text

nth = soup.select('#lottoDrwNo')[0].text
  
print('{0} ({1}) 당첨 번호는 {2} 입니다.'.format(nth,date,ns[0:6]))
print('보너스 번호는 {0} 입니다.'.format(ns[6]))


url = 'https://m.dhlottery.co.kr/gameResult.do?method=byWin'
response = requests.get(url).text
soup2 = BeautifulSoup(response, 'html.parser')
getn = soup2.select('#container > div > div.tbl_basic > table > tbody > tr')

ready =[]
ready.append(getn[1])
ready.append(getn[4])
ready.append(getn[6])

money = []
for a in ready:
  money.append(a.select('strong')[0].text)

print('당첨 금액은 1등 {0}, 2등 {1}, 3등 {2}, 4등 50,000 원, 5등 5,000 원 입니다.'.format(money[0],money[1],money[2]))
```



### 점심메뉴

```python
import random

menu = ['순남시레기', '멀티캠퍼스 20층', '양자감',
        '강남목장', '시골집', '이자카야 단']
menu_detail = {"순남시레기":"시레기국, 보쌈",
               "멀티캠퍼스 20층":"오늘의 메뉴",
               "양자감":"차돌짬뽕",
               "강남목장":"뚝배기 불고기", 
               "시골집":"쌈밥정식",
               "이자카야 단":"오야꼬동"}

lunch = random.choice(menu)
print("오늘의 식사 장소는 " + lunch + " 입니다.")
print(lunch + "에서는 " + menu_detail[lunch] 
      + "를 추천합니다.")
```



### 미세먼지

```python
import requests
from bs4 import BeautifulSoup

url = 'http://openapi.airkorea.or.kr/openapi/services/rest/ArpltnInforInqireSvc/getCtprvnRltmMesureDnsty?sidoName=%EC%84%9C%EC%9A%B8&ServiceKey={}&ver=1.3&pageNo=3'.format(key)
response = requests.get(url).text
soup = BeautifulSoup(response, 'xml')
gn = soup('item')[7]
location = gn.stationName.text
time = gn.dataTime.text
dust = int(gn.pm10Value.text)

print('{0} 기준: 서울시 {1}의 미세먼지 농도는 {2} 입니다.'.format(time, location, dust))

condition = ""
if(dust > 150):
  condition = "매우 나쁨"
elif(dust > 60):
  condition = "나쁨"
elif(dust > 30):
  condition = "보통"
else:
  condition = "좋음"

print("오늘의 미세먼지 단계는 {0}입니다.".format(condition))

```



### 먹보

```python
dish = ['삼겹살', '스테이크', '순두부', '라면', '계란말이', '김치전', '파전']

sz = len(dish)

for food in dish:
  print(food + "먹었다.")
  
for food in dish:
  n=2
  while n>0 :
    print(food + "먹었다.")
    n -= 1
```



### 총인구수

```python
import requests
from bs4 import BeautifulSoup

url = 'http://kosis.kr/conts/nsportalStats/nsportalStats_0102Body.jsp;jsessionid=PL7tm9exauzEsoeadUcl14CmFKoiakPnQ37BSg4xPQdK1zEjUMGVwEjwZ0oPPq4D.STAT_SIGA1_servlet_engine4?menuId=10&NUM=1001'
response = requests.get(url).text
soup = BeautifulSoup(response, 'html.parser')

dt = soup.find('div',{'id':'Header'})
date = str(dt)
date = date[date.find("span")+5:date.find("현재")+2]

txt = soup.find('ul', {'id':'menuTop10'})
total_n = str(txt)
total_n = total_n[total_n.find("총인구수"):]
total_n = total_n[total_n.find("textRight")+11:]
total_n = total_n[:total_n.find("명")+1]

print("우리나라의 {0} 총 인구수는 {1} 입니다.".format(date, total_n))
```



### 기대수명

```python
import requests
from bs4 import BeautifulSoup

url = 'http://kosis.kr/conts/nsportalStats/nsportalStats_0102Body.jsp;jsessionid=PL7tm9exauzEsoeadUcl14CmFKoiakPnQ37BSg4xPQdK1zEjUMGVwEjwZ0oPPq4D.STAT_SIGA1_servlet_engine4?menuId=10&NUM=1001'
response = requests.get(url).text
soup = BeautifulSoup(response, 'html.parser')

dt = soup.find('div',{'id':'Header'})
date = str(dt)
date = date[date.find("span")+5:date.find("현재")+2]

txt = soup.find('ul', {'id':'menuTop10'})
total_n = str(txt)
total_n = total_n[total_n.find("기대수명"):]
total_n = total_n[total_n.find("textRight")+11:]
total_n = total_n[:total_n.find("년")+1]

print("우리나라의 {0} 기대수명은 {1} 입니다.".format(date,total_n))
```



### 날씨

```python
import requests

url = "https://api.openweathermap.org/data/2.5/weather?q=Seoul,kr&lang=kr&APPID="+key

data = requests.get(url).json()
weather = data['weather'][0]['description']
temp = float(data['main']['temp'])-273.15
temp_min = float(data['main']['temp_min'])-273.15
temp_max = float(data['main']['temp_max'])-273.15

wind = data['wind']['speed']
visibility = data['visibility'] / 1000

print("""서울의 오늘 날씨는 [{}] 이며, 섭씨 {:.1f}도 입니다.
최저/최고 온도는 {:.1f}/{:.1f}도 입니다.
풍속은 {:.1f} m/s 입니다.
가시거리는 {} km 입니다.
""".format(weather, temp, temp_min, temp_max, wind, visibility)
)
```



### 로또예상

```python
import random
import requests
from bs4 import BeautifulSoup

## 랜덤하게 번호 샘플링
arr = list(range(1,46))
lottoex = []
lottoex = random.sample(arr,6)
lottoex.sort()
print('예상 로또 번호는 {0} 입니다.'.format(lottoex))

## 당첨 번호 크롤링
url = 'http://www.lotto.co.kr/'
response = requests.get(url).text
soup = BeautifulSoup(response, 'html.parser')
test = soup.find('div', {'class' : 'current_result'})
txt = str(test)

n = txt[txt.find('strong')+7:]
n = n[:n.find('strong')-2]

date = txt[txt.find('추첨')-11:txt.find('추첨')-1]

lotto = []
for i in range(6):
  s1 = txt[txt.find("on/")+3:txt.find(".png")]
  lotto.append(int(s1))
  txt = txt[txt.find(".png")+4:]
  
print('{0}회 (추첨일 {1}) 당첨 번호는 {2} 입니다.'.format(n,date,lotto))

s2 = txt[txt.find("bonus/")+6:]
s2 = s2[:s2.find(".png")]
bonus = int(s2)
print('보너스 번호는 {0} 입니다.'.format(s2))

## 일치 갯수 확인
cnt = 0
for x in lotto:
  for y in lottoex:
    if x == y:
      cnt += 1

   
## 당첨 금액 크롤링      
url = 'https://m.dhlottery.co.kr/gameResult.do?method=byWin'
response = requests.get(url).text
soup2 = BeautifulSoup(response, 'html.parser')
getn = soup2.select('#container > div > div.tbl_basic > table > tbody > tr')

money1 = getn[1]
money2 = getn[4]
money3 = getn[6]


## 결과 출력
result = ''
money = '0원'
ck = False
if(cnt == 6):
  result = '1등'
  a = money1.select('strong')
  money = a[0].text
elif(cnt == 5):
  for x in lottoex:
    if x == bonus:
      ck = True
      break
  if ck :
    result = '2등'
    a = money2.select('strong')
    money = a[0].text
  else :
    result = '3등'
    a = money3.select('strong')
    money = a[0].text
elif(cnt == 4) :
  result = '4등'
  money = '50,000원'
elif(cnt == 3):
  result = '5등'
  money = '5,000원'
else :
  result = '꽝'
  
print('일치 개수는 {0}개 입니다. 결과는 {1} 입니다.'.format(cnt,result))
print('당첨 금액은 {0} 입니다.'.format(money))
```



### 네이버웹툰

```python
import requests
from bs4 import BeautifulSoup as bs
from datetime import date

daylist = ['mon','tue','wed','thu','fri','sat','sun']
daylistforex = ['월','화','수','목','금','토','일']
today = date.today()
day = daylist[today.weekday()]
dayforex = daylistforex[today.weekday()]
url = 'http://comic.naver.com/webtoon/weekdayList.nhn?week=' + day + '/'

response = requests.get(url).text
soup = bs(response, 'html.parser')

toons = []
lstlink = []
wttitle = []
imglink = []
baseurl = 'http://comic.naver.com'

wtlist = soup.find_all('div', {'class':'thumb'})

notdp = 3
for x in wtlist:
  if notdp > 0 :
    notdp -= 1
    continue
  
  test = str(x)
  dic = {}
  
  tmp = test[test.find('href=')+6:]
  tmp = tmp[:tmp.find('"')]
  lstlink.append(baseurl + tmp)
  
  tmp2 = test[test.find('this.src=')+9:]
  tmp2 = tmp2[tmp2.find('src=')+5:tmp2.find('title=')-2]
  imglink.append(tmp2)
  
  tmp3 = test[test.find('title=')+7:]
  tmp3 = tmp3[:tmp3.find('>')-1]
  wttitle.append(tmp3)
  
  dic['title'] = tmp3
  dic['link'] = tmp
  dic['img'] = tmp2
  toons.append(dic)

print('오늘의 '+ dayforex +'요 웹툰 최신화 보러가기')
#print(toons)

for i in range(len(wttitle)):
  print(wttitle[i])
  print(lstlink[i])
  print(imglink[i])
```



### 네이버웹툰2

```python
import requests
from bs4 import BeautifulSoup as bs
from datetime import date

daylist = ['mon','tue','wed','thu','fri','sat','sun']
daylistforex = ['월','화','수','목','금','토','일']
today = date.today()
day = daylist[today.weekday()]
dayforex = daylistforex[today.weekday()]

url = 'http://comic.naver.com/webtoon/weekdayList.nhn?week=' + day + '/'
response = requests.get(url).text
soup = bs(response, 'html.parser')

toons = []
# img_list 의 li 를 선택
# 만약 모든 정보를 담고있지 못하다면 더 큰 범위에서 추출해야 한다.
li = soup.select('.img_list li')

for item in li:
  toon = {
    # dt의 a태그를 선택하겠다.
    'title' : item.select('dt a')[0].text,
    # [0] 뒤의 ["#"]는 속성을 의미한다.
    'url' : item.select('dt a')[0]["href"],
    # thumb라는 div class에 있는 img의 src 속성을 추출
    'img' : item.select('.thumb img')[0]["src"]
    # 검색할 때 class로 검색하면 " .클래스이름 "
    # id로 검색하면 " #id이름 " 으로 검색한다.
    # 이후 속성은 dictionary와 같이 뽑아낼 수 있다.
  }
  toons.append(toon)

print(dayforex + '요 웹툰 보러가기')  
for item in toons:
  #print(item['img'])
  print(item['title'])
  print(item['url'])
```



### 다음웹툰

```python
import requests
import json
from datetime import date

daylist = ['mon','tue','wed','thu','fri','sat','sun']
daylistforex = ['월','화','수','목','금','토','일']
today = date.today()
day = daylist[today.weekday()]
dayforex = daylistforex[today.weekday()]

url = 'http://webtoon.daum.net/data/pc/webtoon/list_serialized/' + day
response = requests.get(url).text
output = json.loads(response)
docu = output['data']

toons = []
baseurl = 'http://webtoon.daum.net/webtoon/view/'
for x in docu:
  toon ={
    'title' : x['title'],
    'url' : baseurl + x['nickname'],
    'img' : x['thumbnailImage2']['url']
  }
  toons.append(toon)

print(dayforex + '요일 다음 웹툰 바로 보기')
for x in toons:
  #print(x['img'])
  print(x['title'])
  print(x['url'])
```

